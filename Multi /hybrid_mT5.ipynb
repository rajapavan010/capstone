{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ff1307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e9e56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"telugu\", \"urdu\", \"marathi\", \"hindi\", \"tamil\", \"bengali\", \"english\"]\n",
    "dfs = []\n",
    "for lang in languages:\n",
    "    dataset = load_dataset(\"csebuetnlp/xlsum\", lang, split=\"train[:2000]\")\n",
    "    df = dataset.to_pandas()\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31248fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 5)\n",
      "                       id                                                url  \\\n",
      "0  international-53649907  https://www.bbc.com/telugu/international-53649907   \n",
      "1          india-46550604          https://www.bbc.com/telugu/india-46550604   \n",
      "2          india-43404438          https://www.bbc.com/telugu/india-43404438   \n",
      "3  international-54671956  https://www.bbc.com/telugu/international-54671956   \n",
      "4                53723894                https://www.bbc.com/telugu/53723894   \n",
      "\n",
      "                                               title  \\\n",
      "0  పాకిస్తాన్ ఎయిర్‌లైన్స్‌లో నకిలీ లైసెన్సుల పైల...   \n",
      "1  తెలంగాణ ముఖ్యమంత్రిగా కేసీఆర్ రెండోసారి ప్రమాణ...   \n",
      "2  ‘అధికారం కొన్ని కులాల గుప్పిట్లోనే ఉండాలా? కుద...   \n",
      "3  పోలండ్‌లో కొత్త అబార్షన్ చట్టాలను వ్యతిరేకిస్త...   \n",
      "4  దిల్లీ అల్లర్లపై పరస్పర విరుద్ధ నివేదికలు... ఏ...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  పాకిస్తాన్ విమానయాన రంగంలో కొత్త సంక్షోభం మొదల...   \n",
      "1  తెలంగాణ ముఖ్యమంత్రిగా కల్వకుంట్ల చంద్రశేఖర్ రా...   \n",
      "2  గుంటూరులో జనసేన ఆవిర్భావ దినోత్సవ సభ జరిగింది....   \n",
      "3  పోలండ్‌లోని కొత్త అబార్షన్ చట్టాలకు వ్యతిరేకంగ...   \n",
      "4  ఈ ఏడాది ఫిబ్రవరిలో జరిగిన ఢిల్లీ హింసలో ఏ వర్గ...   \n",
      "\n",
      "                                                text  \n",
      "0  అయితే, ఆ జాబితా తప్పులతడకని పైలట్లు అంటున్నారు...  \n",
      "1  ఆ తరువాత ఆయన రెండోసారి ముఖ్యమంత్రిగా బాధ్యతలు ...  \n",
      "2  ప్రత్యేక హోదా సాధన, టీడీపీపై విమర్శలే ప్రధానాస...  \n",
      "3  గత ఏడాది చట్టబద్ధంగా జరిగిన అబార్షన్లలో 98% కడ...  \n",
      "4  అయితే, ఈ అంశంపై సొంతంగా విచారణ జరిపిన ఈ కమిటీల...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(dfs, ignore_index=True)\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6974c1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "C:\\Users\\SRMAPCSELAB2022-147\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "\n",
    "model_id = \"csebuetnlp/mT5_multilingual_XLSum\"\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846f5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = AutoModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73ff20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank_bert(text, num_sentences=3):\n",
    "    sentences = text.split('.')\n",
    "    inputs = bert_tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    return \" \".join([sentence for _, sentence in ranked_sentences[:num_sentences]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efbe14fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, text_max_token_len=200, summary_max_token_len=12):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.text_max_token_len = text_max_token_len\n",
    "        self.summary_max_token_len = summary_max_token_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        data_row = self.data.iloc[index]\n",
    "        text = data_row['text']\n",
    "\n",
    "        # Extractive summarization using TextRank with BERT\n",
    "        extracted_summary = textrank_bert(text)\n",
    "\n",
    "        # Tokenize extracted summary (input to MT5 model)\n",
    "        text_encoding = tokenizer(\n",
    "            extracted_summary,\n",
    "            max_length=self.text_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Tokenize target summary (ground truth)\n",
    "        summary_encoding = tokenizer(\n",
    "            data_row['summary'],\n",
    "            max_length=self.summary_max_token_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = summary_encoding['input_ids']\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return dict(\n",
    "            input_ids=text_encoding['input_ids'].flatten(),\n",
    "            attention_mask=text_encoding['attention_mask'].flatten(),\n",
    "            labels=labels.flatten(),\n",
    "            decoder_attention_mask=summary_encoding['attention_mask'].flatten()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c43938",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = HybridSummaryDataset(data=df_train, tokenizer=tokenizer)\n",
    "test_dataset = HybridSummaryDataset(data=df_test, tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=10)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc26442f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SRMAPCSELAB2022-147\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93df1b10ee364e75a7a78225dd439e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 -- Loss: 3.5863237380981445\n",
      "Epoch: 2 -- Loss: 2.4182894229888916\n",
      "Epoch: 3 -- Loss: 2.1674599647521973\n",
      "Epoch: 4 -- Loss: 1.6196531057357788\n",
      "Epoch: 5 -- Loss: 1.1678788661956787\n",
      "Epoch: 6 -- Loss: 0.789129912853241\n",
      "Epoch: 7 -- Loss: 0.4748256504535675\n",
      "Epoch: 8 -- Loss: 0.4586552083492279\n",
      "Epoch: 9 -- Loss: 0.2479063868522644\n",
      "Epoch: 10 -- Loss: 0.2037576138973236\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    print(f\"Epoch: {epoch + 1} -- Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9100560",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_references = []\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], max_length=12)\n",
    "    decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "    all_predictions.extend(decoded_preds)\n",
    "    all_references.extend(decoded_labels)\n",
    "\n",
    "rouge_score = rouge_metric.compute(predictions=all_predictions, references=all_references)\n",
    "bleu_score = bleu_metric.compute(predictions=all_predictions, references=all_references)\n",
    "\n",
    "print(\"ROUGE Score:\", rouge_score)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c14007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
